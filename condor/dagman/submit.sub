#################################################################
# Another shell script that set up the settings
# for your dag, including python scripts/arguments,
# where err/log/out are dumped, memories/disks/CPU
# resquest, etc.
#
# By default,
# In cobalt, time limit is 2 days, 2 GB (?).
# In umd condor, no time limit, 1 GB
#
# NOTE:: MAKE SURE NONE OF THE PATHS HERE POINTS TO I3HOME
#        ALL PATHS MUST POINT TO EITHER /data/condor_build/ OR
#        /data/i3store0/ OR /data/i3scratch0/
################################################################

## Jobs requiring more than 1GB of memory or 1CPU can add:
#request_cpus = <num_cpus>
#request_memory = 5
#<mem_in_MB> (default 1GB in umd, 2GB in wisc)
#request_disk = <disk_in_KB>

# Run the environment script from icetray before anything else
#executable = /data/condor_builds/users/blaufuss/combo/stable/build/env-shell.sh
executable = /cvmfs/icecube.opensciencegrid.org/py3-v4.1.0/Ubuntu_18.04_x86_64/metaprojects/combo/V01-00-00/env-shell.sh
# Where the log, out, err files will live
basedir     = /data/condor_builds/users/blaufuss/umd_computing_examples/condor/dagman/
output      = $(basedir)$(Jobname).$(Cluster).out
error       = $(basedir)$(Jobname).$(Cluster).err
should_transfer_files = YES

# Only 1 log file for all jobs
log = /data/condor_builds/users/blaufuss/umd_computing_examples/condor/dagman/$(Jobname).log

# Other condor stuff
notification   = never
getenv         = true
universe       = vanilla

# Submit !
Arguments = $(command)
queue

